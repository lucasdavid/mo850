{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Lucas David](http://github.com/lucasdavid)  \n",
    "This notebook can be downloaded at https://github.com/lucasdavid/mo850/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import statsmodels.stats.contingency_tables\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpaired Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following datasets will be loaded: a.csv b.csv c.csv d.csv e.csv\n",
      "Sample of a loaded dataset:\n",
      "    measure  group\n",
      "0  4.249030      0\n",
      "1  5.542826      0\n",
      "2  5.161981      0\n",
      "3  2.267553      0\n",
      "4  4.155343      0\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data/3/'\n",
    "\n",
    "datasets = [letter + '.csv' for letter in 'abcde']\n",
    "print('The following datasets will be loaded:', *datasets)\n",
    "\n",
    "datasets = [pd.read_csv(os.path.join(data_dir, d), header=None, names=['measure'])\n",
    "            for d in datasets]\n",
    "\n",
    "for index, d in enumerate(datasets):\n",
    "    d['group'] = index\n",
    "\n",
    "print('Sample of a loaded dataset:',\n",
    "      datasets[0].head(),\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for anova test: 3.3595478270572274e-12\n"
     ]
    }
   ],
   "source": [
    "s, p = stats.f_oneway(*(d['measure'] for d in datasets))\n",
    "print('p-value for anova test:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD,FWER=0.05\n",
      "============================================\n",
      "group1 group2 meandiff  lower  upper  reject\n",
      "--------------------------------------------\n",
      "  0      1    -0.5373   -1.514 0.4395 False \n",
      "  0      2     1.831    0.8718 2.7901  True \n",
      "  0      3     1.908    1.0037 2.8123  True \n",
      "  0      4     1.7006   0.7414 2.6597  True \n",
      "  1      2     2.3682   1.3405 3.396   True \n",
      "  1      3     2.4453   1.4685 3.422   True \n",
      "  1      4     2.2379   1.2101 3.2656  True \n",
      "  2      3     0.077   -0.8821 1.0362 False \n",
      "  2      4    -0.1304  -1.1414 0.8807 False \n",
      "  3      4    -0.2074  -1.1665 0.7518 False \n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "merged_datasets = pd.concat(datasets)\n",
    "r = pairwise_tukeyhsd(merged_datasets['measure'],\n",
    "                      groups=merged_datasets['group'])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for Kruskal-Wallis: 2.275373540934311e-09\n"
     ]
    }
   ],
   "source": [
    "s, p = stats.kruskal(*(d['measure'] for d in datasets))\n",
    "print('p-value for Kruskal-Wallis:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   p    p holm  p bonferroni\n",
      "pairs                                       \n",
      "(a, b)  2.433450e-01  0.973380      1.000000\n",
      "(a, c)  1.332963e-04  0.000800      0.001333\n",
      "(a, d)  5.508804e-06  0.000050      0.000055\n",
      "(a, e)  1.515892e-04  0.000800      0.001516\n",
      "(b, c)  6.598469e-06  0.000053      0.000066\n",
      "(b, d)  9.583666e-07  0.000010      0.000010\n",
      "(b, e)  2.789325e-05  0.000195      0.000279\n",
      "(c, d)  5.452595e-01  1.000000      1.000000\n",
      "(c, e)  8.505281e-01  1.000000      1.000000\n",
      "(d, e)  5.883647e-01  1.000000      1.000000\n"
     ]
    }
   ],
   "source": [
    "pairs_indices = list(combinations('abcde', 2))\n",
    "pairs_data = combinations([d['measure'] for d in datasets], 2)\n",
    "\n",
    "ps = [stats.ranksums(x, y)[1] for x, y in pairs_data]\n",
    "ps_holm = multipletests(ps, method='holm')[1]\n",
    "ps_bonferroni = multipletests(ps, method='bonferroni')[1]\n",
    "\n",
    "ps = pd.DataFrame({\n",
    "    'pairs': pairs_indices,\n",
    "    'p': ps,\n",
    "    'p holm': ps_holm,\n",
    "    'p bonferroni': ps_bonferroni\n",
    "}).set_index('pairs')[['p', 'p holm', 'p bonferroni']]\n",
    "\n",
    "\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following datasets will be loaded: multi.csv\n",
      "           0          1          2           3           4\n",
      "0  34.381581  38.230745  52.236630   59.027814   28.288420\n",
      "1  78.475309  74.647168  82.374582   95.970556   28.983943\n",
      "2   5.676301   8.919621  16.492051   28.646045   29.585645\n",
      "3  90.357392  90.869337  98.987833  112.835174  118.070135\n",
      "4  72.198253  71.068576  79.561990   92.640627   72.266390\n"
     ]
    }
   ],
   "source": [
    "dataset = 'multi.csv'\n",
    "print('The following datasets will be loaded:', dataset)\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(data_dir, dataset), header=None)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87340fd863a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfriedmanchisquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p-value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stats' is not defined"
     ]
    }
   ],
   "source": [
    "s, p = stats.friedmanchisquare(*(dataset[c] for c in dataset.columns))\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Homework for ML Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1024) [[ 0.59055118  0.58267717  0.5984252  ...  0.09448819  0.05249344\n",
      "  -0.0183727 ]\n",
      " [ 0.43307087  0.44619423  0.46981627 ... -0.96587927 -0.99475066\n",
      "  -1.        ]\n",
      " [-0.09711286 -0.09973753 -0.02099738 ...  0.09448819  0.28346457\n",
      "   0.31496063]]\n",
      "(3000,) [4 8 2]\n",
      " data statistics: -0.04883058135799428 0.48773118463802934\n",
      "label statistics: {0: 298, 1: 324, 2: 283, 3: 306, 4: 294, 5: 294, 6: 287, 7: 316, 8: 300, 9: 298}\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets.cifar10 import load_data\n",
    "\n",
    "samples_used = 3000\n",
    "\n",
    "(x, y), _ = load_data()\n",
    "x = x.astype(float)\n",
    "x /= 127.\n",
    "x -= 1.\n",
    "\n",
    "# Gray-scale, 1-rank tensors.\n",
    "x = x.mean(axis=-1).reshape(x.shape[0], -1)\n",
    "y = y.ravel()\n",
    "\n",
    "p = np.random.permutation(x.shape[0])[:samples_used]\n",
    "x, y = x[p], y[p]\n",
    "\n",
    "print(x.shape, x[:3])\n",
    "print(y.shape, y[:3])\n",
    "\n",
    "print(' data statistics:', x.mean(), x.std())\n",
    "print('label statistics:', dict(zip(*np.unique(y, return_counts=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(estimator):\n",
    "    if estimator == 'a':\n",
    "        return SVC()\n",
    "    elif estimator == 'b':\n",
    "        return RandomForestClassifier()\n",
    "    else:\n",
    "        raise ValueError('unknown estimator of type `%s`' % estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing experiment 0\n",
      "Executing experiment 1\n",
      "Executing experiment 2\n",
      "Executing experiment 3\n",
      "Executing experiment 4\n"
     ]
    }
   ],
   "source": [
    "r = np.random.RandomState(42)\n",
    "\n",
    "experiments = 5\n",
    "scores = []\n",
    "\n",
    "for experiment in range(experiments):\n",
    "    print('Executing experiment', experiment)\n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=r)\n",
    "    \n",
    "    for train_indices, test_indices in skf.split(x, y):\n",
    "        x_train, y_train = (d[train_indices] for d in (x, y))\n",
    "        x_test, y_test = (d[test_indices] for d in (x, y))\n",
    "\n",
    "        for estimator in 'ab':\n",
    "            e = build_model(estimator)\n",
    "            e.fit(x_train, y_train)\n",
    "            scores += [e.score(x_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model a's average score: 0.3078667146074287\n",
      "model b's average score: 0.2315298362354828\n",
      "On average, model a performed better.\n"
     ]
    }
   ],
   "source": [
    "scores = np.asarray(scores)\n",
    "scores_a = scores[::2]\n",
    "scores_b = scores[1::2]\n",
    "\n",
    "print('model a\\'s average score:', scores_a.mean())\n",
    "print('model b\\'s average score:', scores_b.mean())\n",
    "\n",
    "print('On average, model',\n",
    "      'a' if scores_a.mean() > scores_b.mean() else 'b',\n",
    "      'performed better.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model a is averagely better, with a p-value of 0.005062032126267864\n"
     ]
    }
   ],
   "source": [
    "s, p = stats.wilcoxon(scores_a, scores_b)\n",
    "print('p-value from wilconxon test:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc_table = pd.crosstab(scores_a, scores_b)\n",
    "# print(rc_table)\n",
    "# p = statsmodels.stats.contingency_tables.mcnemar(rc_table).pvalue\n",
    "# print('p-value from mcnemar test:', p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
